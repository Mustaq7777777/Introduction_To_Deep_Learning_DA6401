{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustaq7777777/Introduction_To_Deep_Learning_DA6401/blob/main/Assignment1/DeepLearning_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np9y5M3mmoPL"
      },
      "outputs": [],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 1"
      ],
      "metadata": {
        "id": "ZEMlJmiIHUGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "#There are 10 class labels present in mnist data set.\n",
        "class_labels = [\n",
        "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
        "]\n",
        "\n",
        "\n",
        "# wandb.init(project=\"DA6401-Assignment1\", name=\"run1\", reinit=True)\n",
        "\n",
        "# #Loading data from the fashion_mnist\n",
        "# (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# #storing images in Image_set\n",
        "# images_to_log = []\n",
        "# for class_index in range(10):\n",
        "\n",
        "#     #Find the instance of each class and append it .\n",
        "#     idx = np.where(train_labels == class_index)[0][0]\n",
        "\n",
        "\n",
        "#     image = train_images[idx]\n",
        "#     wandb_image = wandb.Image(image, caption=class_labels[class_index])\n",
        "#     images_to_log.append(wandb_image)\n",
        "\n",
        "# # Log the image to WandB\n",
        "# wandb.log({\"Question1 Sample images for each class\": images_to_log})\n"
      ],
      "metadata": {
        "id": "bU_YG1OUkeXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "Svl549ZQNN7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "eXRsP47Cv3QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)/256\n",
        "x_test = x_test.reshape(10000, 784)/256\n",
        "\n",
        "y_train = y_train.reshape(60000, 1)\n",
        "y_test = y_test.reshape(10000, 1)"
      ],
      "metadata": {
        "id": "pJ5i9fuev6CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions and their derivatives"
      ],
      "metadata": {
        "id": "n2kZHn46xC4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def softmax(v):\n",
        "    exp_vector = np.exp(v - np.max(v))\n",
        "    return exp_vector / np.sum(exp_vector)\n",
        "\n",
        "# Sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Tanh\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "# ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def d_relu(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# One-hot vector\n",
        "def e(l, length):\n",
        "    y = np.zeros([length, 1])\n",
        "    y[int(l)] = 1\n",
        "    return y"
      ],
      "metadata": {
        "id": "4EKnZQSGLG1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization Of Weights and Biases"
      ],
      "metadata": {
        "id": "il0WyZkrHLyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def init(W, b, input_nodes, hiddenlayers, hiddennodes, output_nodes, initializer):\n",
        "\n",
        "    np.random.seed(1)\n",
        "    if initializer == \"Xavier\":\n",
        "        # Xavier initialization for the first hidden layer (input to first hidden layer)\n",
        "        W[1] = np.random.normal(0.0, np.sqrt(1.0 / input_nodes), (hiddennodes, input_nodes))\n",
        "        b[1] = np.zeros((hiddennodes, 1))\n",
        "        # Xavier initialization for subsequent hidden layers\n",
        "        for i in range(2, hiddenlayers + 1):\n",
        "            W[i] = np.random.normal(0.0, np.sqrt(1.0 / hiddennodes), (hiddennodes, hiddennodes))\n",
        "            b[i] = np.zeros((hiddennodes, 1))\n",
        "        # Xavier initialization for the output layer (from last hidden layer to output)\n",
        "        W[hiddenlayers + 1] = np.random.normal(0.0, np.sqrt(1.0 / hiddennodes), (output_nodes, hiddennodes))\n",
        "        b[hiddenlayers + 1] = np.zeros((output_nodes, 1))\n",
        "    elif initializer == \"random\":\n",
        "        # Random initialization for the first hidden layer (input to first hidden layer)\n",
        "        W[1] = np.random.rand(hiddennodes, input_nodes) - 0.5\n",
        "        b[1] = np.zeros((hiddennodes, 1))\n",
        "        # Random initialization for subsequent hidden layers\n",
        "        for i in range(2, hiddenlayers + 1):\n",
        "            W[i] = np.random.rand(hiddennodes, hiddennodes) - 0.5\n",
        "            b[i] = np.zeros((hiddennodes, 1))\n",
        "        # Random initialization for the output layer (from last hidden layer to output)\n",
        "        W[hiddenlayers + 1] = np.random.rand(output_nodes, hiddennodes) - 0.5\n",
        "        b[hiddenlayers + 1] = np.zeros((output_nodes, 1))\n",
        "\n",
        "    return W, b\n",
        "\n"
      ],
      "metadata": {
        "id": "ykuw6EsQN1Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation"
      ],
      "metadata": {
        "id": "pyTeztydTGPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For applying activation function\n",
        "def g(z, act_func):\n",
        "    if act_func == \"sigmoid\":\n",
        "        return sigmoid(z)\n",
        "    elif act_func == \"tanh\":\n",
        "        return tanh(z)\n",
        "    elif act_func == \"relu\":\n",
        "        return relu(z)\n",
        "\n",
        "# For applying derivative of activation function\n",
        "def g_prime(z, act_func):\n",
        "    if act_func == \"sigmoid\":\n",
        "        return d_sigmoid(z)\n",
        "    elif act_func == \"tanh\":\n",
        "        return d_tanh(z)\n",
        "    elif act_func == \"relu\":\n",
        "        return d_relu(z)\n",
        "\n",
        "# For calculating output using softmax\n",
        "def o(z, act_func):\n",
        "    if act_func == \"softmax\":\n",
        "        return softmax(z)\n",
        "\n",
        "# Applying forward propagation for the L-th training example\n",
        "def forward_propagation(x_data, hidden_layers, hidden_nodes, input_nodes, output_nodes, W, b, a, h, L, act_func):\n",
        "    h[0] = x_data[L].reshape(-1, 1)\n",
        "    for layer in range(1, hidden_layers + 1):\n",
        "        a[layer] = np.dot(W[layer], h[layer - 1]) + b[layer]\n",
        "        h[layer] = g(a[layer], act_func)\n",
        "    a[hidden_layers + 1] = np.dot(W[hidden_layers + 1], h[hidden_layers]) + b[hidden_layers + 1]\n",
        "    ycap = o(a[hidden_layers + 1], \"softmax\")\n",
        "    return ycap, a, h\n"
      ],
      "metadata": {
        "id": "VlVPL6vpTJ4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back Propagation"
      ],
      "metadata": {
        "id": "ir9Khj8CY3u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                     W, b, a, h, VW, Vb, sample_idx, yhat, activation_func, loss_func):\n",
        "\n",
        "    output_layer = hiddenlayers + 1\n",
        "    delta = {}\n",
        "\n",
        "    # delta[outputlayer] computation based on loss function\n",
        "    if loss_func == \"cross_entropy\":\n",
        "        delta[output_layer] = yhat - e(y_train[sample_idx].item(), output_nodes)\n",
        "    elif loss_func == \"mean_squared_error\":\n",
        "        target = e(y_train[sample_idx].item(), output_nodes)\n",
        "        # Compute the Jacobian matrix of the softmax output.\n",
        "        jacobian = np.diagflat(yhat) - np.dot(yhat, yhat.T)\n",
        "        # Apply the chain rule and scale by the number of output nodes (for the mean).\n",
        "        delta[output_layer] = np.dot(jacobian, (yhat - target)) / output_nodes\n",
        "\n",
        "    # propagating backwards by computing derivatives at each layer till the input layer\n",
        "    for k in range(output_layer, 0, -1):\n",
        "        grad_W = np.dot(delta[k], h[k-1].T)\n",
        "        grad_b = delta[k]\n",
        "        if k in VW:\n",
        "            VW[k] += grad_W\n",
        "            Vb[k] += grad_b\n",
        "        else:\n",
        "            VW[k] = grad_W\n",
        "            Vb[k] = grad_b\n",
        "        if k > 1:\n",
        "            prev_error = np.dot(W[k].T, delta[k])\n",
        "            delta[k-1] = np.multiply(prev_error, g_prime(a[k-1], activation_func).reshape(-1, 1))\n",
        "    return VW, Vb"
      ],
      "metadata": {
        "id": "YVkIsRrwY8Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "n6Fs8ITL0kzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function (for cross-entropy and mean_squared_error)\n",
        "def loss(yhat, y_train, loss_func, sample_idx, output_nodes):\n",
        "    if loss_func == \"cross_entropy\":\n",
        "        return -np.log(yhat[y_train[sample_idx].item()])\n",
        "    elif loss_func == \"mean_squared_error\":\n",
        "        target = e(y_train[sample_idx].item(), output_nodes)\n",
        "        return np.mean(np.square(yhat - target))"
      ],
      "metadata": {
        "id": "sWj5Vvyowdfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating Performance and Plotting"
      ],
      "metadata": {
        "id": "Ha3GKZ_w_Wza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performance(x_test, y_test, x_train, y_train, W, b,\n",
        "                         hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                         activation_func, loss_func):\n",
        "    # Test accuracy.\n",
        "    correct_test = 0\n",
        "    for idx in range(len(x_test)):\n",
        "        yhat, _, _ = forward_propagation(x_test, hidden_layers, hidden_nodes, input_nodes,\n",
        "                                         output_nodes, W, b, {}, {}, idx, activation_func)\n",
        "        pred = int(np.argmax(yhat))\n",
        "        if pred == y_test[idx]:\n",
        "            correct_test += 1\n",
        "    test_accuracy = (correct_test / len(x_test)) * 100\n",
        "    wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "    # Validation on last 6000 training samples.\n",
        "    correct_val = 0\n",
        "    total_val_loss = 0\n",
        "    for idx in range(54000, 60000):\n",
        "        yhat, _, _ = forward_propagation(x_train, hidden_layers, hidden_nodes, input_nodes,\n",
        "                                         output_nodes, W, b, {}, {}, idx, activation_func)\n",
        "        total_val_loss += loss(yhat, y_train, loss_func, idx, output_nodes)\n",
        "        pred = int(np.argmax(yhat))\n",
        "        if pred == y_train[idx]:\n",
        "            correct_val += 1\n",
        "    val_accuracy = (correct_val / (60000 - 54000)) * 100\n",
        "    val_loss = total_val_loss / (60000 - 54000)\n",
        "    wandb.log({\"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "\n",
        "    # Training on first 54000 samples.\n",
        "    correct_train = 0\n",
        "    total_train_loss = 0\n",
        "    for idx in range(0, 54000):\n",
        "        yhat, _, _ = forward_propagation(x_train, hidden_layers, hidden_nodes, input_nodes,\n",
        "                                         output_nodes, W, b, {}, {}, idx, activation_func)\n",
        "        total_train_loss += loss(yhat, y_train, loss_func, idx, output_nodes)\n",
        "        pred = int(np.argmax(yhat))\n",
        "        if pred == y_train[idx]:\n",
        "            correct_train += 1\n",
        "    train_accuracy = (correct_train / 54000) * 100\n",
        "    train_loss = total_train_loss / 54000\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"train_loss\": train_loss})\n",
        "\n",
        "    return train_accuracy, val_accuracy, test_accuracy"
      ],
      "metadata": {
        "id": "VuyDBn51_bQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "x2GrKfYcqWfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "        W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "        activation_func, loss_func, weight_decay):\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        VW = {}\n",
        "        Vb = {}\n",
        "        total_loss = 0\n",
        "        for L in range(train_set):\n",
        "            yhat, a, h = forward_propagation(x_train, hiddenlayers, hiddennodes, input_nodes,\n",
        "                                             output_nodes, W, b, a, h, L, activation_func)\n",
        "            curr_loss = loss(yhat, y_train, loss_func, L, output_nodes)\n",
        "            total_loss += curr_loss\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, VW, Vb, L, yhat, activation_func, loss_func)\n",
        "            if L % batch_size == 0:\n",
        "                for i in range(1, hiddenlayers + 2):\n",
        "                    W[i] -= learning_rate * (VW[i] + weight_decay * W[i])\n",
        "                    b[i] -= learning_rate * Vb[i]\n",
        "                VW = {}\n",
        "                Vb = {}\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": total_loss})\n",
        "        evaluate_performance(x_test, y_test, x_train, y_train, W, b,\n",
        "                             hiddenlayers, hiddennodes, input_nodes, output_nodes, activation_func, loss_func)"
      ],
      "metadata": {
        "id": "PC-1Vw7yqdru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum Based Gradient Descent"
      ],
      "metadata": {
        "id": "8JG-RSTNqjEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gd(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "                activation_func, loss_func, weight_decay, momentum):\n",
        "    vW = {}\n",
        "    vB = {}\n",
        "    for epoch in range(num_epochs):\n",
        "        VW = {}\n",
        "        Vb = {}\n",
        "        total_loss = 0\n",
        "        for L in range(train_set):\n",
        "            yhat, a, h = forward_propagation(x_train, hiddenlayers, hiddennodes, input_nodes,\n",
        "                                             output_nodes, W, b, a, h, L, activation_func)\n",
        "            total_loss += loss(yhat, y_train, loss_func, L, output_nodes)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, VW, Vb, L, yhat, activation_func, loss_func)\n",
        "            if L % batch_size == 0:\n",
        "                for i in range(1, hiddenlayers + 2):\n",
        "                    if i not in vW:\n",
        "                        vW[i] = np.zeros_like(W[i])\n",
        "                        vB[i] = np.zeros_like(b[i])\n",
        "                    vW[i] = momentum * vW[i] + learning_rate * (VW[i] + weight_decay * W[i])\n",
        "                    vB[i] = momentum * vB[i] + learning_rate * Vb[i]\n",
        "                    W[i] -= vW[i]\n",
        "                    b[i] -= vB[i]\n",
        "                VW = {}\n",
        "                Vb = {}\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": total_loss})\n",
        "        evaluate_performance(x_test, y_test, x_train, y_train, W, b,\n",
        "                             hiddenlayers, hiddennodes, input_nodes, output_nodes, activation_func, loss_func)"
      ],
      "metadata": {
        "id": "aXa9p9Ngqoh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesterov"
      ],
      "metadata": {
        "id": "XWcqvxZ62Xno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gd(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "                activation_func, loss_func, weight_decay, momentum):\n",
        "    vW = {}\n",
        "    vB = {}\n",
        "    for epoch in range(num_epochs):\n",
        "        VW = {}\n",
        "        Vb = {}\n",
        "        total_loss = 0\n",
        "        for L in range(train_set):\n",
        "            W_temp = {}\n",
        "            b_temp = {}\n",
        "            for i in range(1, hiddenlayers + 2):\n",
        "                if i not in vW:\n",
        "                    vW[i] = np.zeros_like(W[i])\n",
        "                    vB[i] = np.zeros_like(b[i])\n",
        "                W_temp[i] = W[i] - momentum * vW[i]\n",
        "                b_temp[i] = b[i] - momentum * vB[i]\n",
        "            yhat, a_temp, h_temp = forward_propagation(x_train, hiddenlayers, hiddennodes, input_nodes,\n",
        "                                                       output_nodes, W_temp, b_temp, a, h, L, activation_func)\n",
        "            total_loss += loss(yhat, y_train, loss_func, L, output_nodes)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W_temp, b_temp, a_temp, h_temp, VW, Vb, L, yhat, activation_func, loss_func)\n",
        "            if L % batch_size == 0:\n",
        "                for i in range(1, hiddenlayers + 2):\n",
        "                    vW[i] = momentum * vW[i] + learning_rate * (VW[i] + weight_decay * W[i])\n",
        "                    vB[i] = momentum * vB[i] + learning_rate * Vb[i]\n",
        "                    W[i] -= vW[i]\n",
        "                    b[i] -= vB[i]\n",
        "                VW = {}\n",
        "                Vb = {}\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": total_loss})\n",
        "        evaluate_performance(x_test, y_test, x_train, y_train, W, b,\n",
        "                             hiddenlayers, hiddennodes, input_nodes, output_nodes, activation_func, loss_func)"
      ],
      "metadata": {
        "id": "It9BsNsV2aEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSprop"
      ],
      "metadata": {
        "id": "hB4pd3ecYHGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "            W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "            activation_func, loss_func, weight_decay, beta, epsilon):\n",
        "    sW = {}\n",
        "    sB = {}\n",
        "    for epoch in range(num_epochs):\n",
        "        VW = {}\n",
        "        Vb = {}\n",
        "        total_loss = 0\n",
        "        for L in range(train_set):\n",
        "            yhat, a_temp, h_temp = forward_propagation(x_train, hiddenlayers, hiddennodes,\n",
        "                                                       input_nodes, output_nodes, W, b, a, h, L, activation_func)\n",
        "            total_loss += loss(yhat, y_train, loss_func, L, output_nodes)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, VW, Vb, L, yhat, activation_func, loss_func)\n",
        "            if L % batch_size == 0:\n",
        "                for i in range(1, hiddenlayers + 2):\n",
        "                    if i not in sW:\n",
        "                        sW[i] = np.zeros_like(W[i])\n",
        "                        sB[i] = np.zeros_like(b[i])\n",
        "                    grad_W = VW[i] + weight_decay * W[i]\n",
        "                    grad_b = Vb[i]\n",
        "                    sW[i] = beta * sW[i] + (1 - beta) * np.square(grad_W)\n",
        "                    sB[i] = beta * sB[i] + (1 - beta) * np.square(grad_b)\n",
        "                    W[i] -= learning_rate * grad_W / (np.sqrt(sW[i]) + epsilon)\n",
        "                    b[i] -= learning_rate * grad_b / (np.sqrt(sB[i]) + epsilon)\n",
        "                VW = {}\n",
        "                Vb = {}\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": total_loss})\n",
        "        evaluate_performance(x_test, y_test, x_train, y_train, W, b,\n",
        "                             hiddenlayers, hiddennodes, input_nodes, output_nodes, activation_func, loss_func)"
      ],
      "metadata": {
        "id": "s6GLWDRYYS63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam"
      ],
      "metadata": {
        "id": "Gi8htOi-YZUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "         W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "         activation_func, loss_func, weight_decay, beta1, beta2, epsilon):\n",
        "    mW = {}\n",
        "    vW = {}\n",
        "    mB = {}\n",
        "    vB = {}\n",
        "    t = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        VW = {}\n",
        "        Vb = {}\n",
        "        total_loss = 0\n",
        "        for L in range(train_set):\n",
        "            yhat, a_temp, h_temp = forward_propagation(x_train, hiddenlayers, hiddennodes,\n",
        "                                                       input_nodes, output_nodes, W, b, a, h, L, activation_func)\n",
        "            total_loss += loss(yhat, y_train, loss_func, L, output_nodes)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, VW, Vb, L, yhat, activation_func, loss_func)\n",
        "            if L % batch_size == 0:\n",
        "                t += 1\n",
        "                for i in range(1, hiddenlayers + 2):\n",
        "                    if i not in mW:\n",
        "                        mW[i] = np.zeros_like(W[i])\n",
        "                        vW[i] = np.zeros_like(W[i])\n",
        "                        mB[i] = np.zeros_like(b[i])\n",
        "                        vB[i] = np.zeros_like(b[i])\n",
        "                    grad_W = VW[i] + weight_decay * W[i]\n",
        "                    grad_b = Vb[i]\n",
        "                    mW[i] = beta1 * mW[i] + (1 - beta1) * grad_W\n",
        "                    mB[i] = beta1 * mB[i] + (1 - beta1) * grad_b\n",
        "                    vW[i] = beta2 * vW[i] + (1 - beta2) * np.square(grad_W)\n",
        "                    vB[i] = beta2 * vB[i] + (1 - beta2) * np.square(grad_b)\n",
        "                    mW_hat = mW[i] / (1 - beta1**t)\n",
        "                    mB_hat = mB[i] / (1 - beta1**t)\n",
        "                    vW_hat = vW[i] / (1 - beta2**t)\n",
        "                    vB_hat = vB[i] / (1 - beta2**t)\n",
        "                    W[i] -= learning_rate * mW_hat / (np.sqrt(vW_hat) + epsilon)\n",
        "                    b[i] -= learning_rate * mB_hat / (np.sqrt(vB_hat) + epsilon)\n",
        "                VW = {}\n",
        "                Vb = {}\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": total_loss})\n",
        "        evaluate_performance(x_test, y_test, x_train, y_train, W, b,\n",
        "                             hiddenlayers, hiddennodes, input_nodes, output_nodes, activation_func, loss_func)"
      ],
      "metadata": {
        "id": "-0hYG1hQYbE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nadam"
      ],
      "metadata": {
        "id": "3ZuQq_lQYiXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "          W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "          activation_func, loss_func, weight_decay, beta1, beta2, epsilon):\n",
        "    mW = {}\n",
        "    vW = {}\n",
        "    mB = {}\n",
        "    vB = {}\n",
        "    t = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        VW = {}\n",
        "        Vb = {}\n",
        "        total_loss = 0\n",
        "        for L in range(train_set):\n",
        "            yhat, a_temp, h_temp = forward_propagation(x_train, hiddenlayers, hiddennodes,\n",
        "                                                       input_nodes, output_nodes, W, b, a, h, L, activation_func)\n",
        "            total_loss += loss(yhat, y_train, loss_func, L, output_nodes)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, VW, Vb, L, yhat, activation_func, loss_func)\n",
        "            if L % batch_size == 0:\n",
        "                t += 1\n",
        "                for i in range(1, hiddenlayers + 2):\n",
        "                    if i not in mW:\n",
        "                        mW[i] = np.zeros_like(W[i])\n",
        "                        vW[i] = np.zeros_like(W[i])\n",
        "                        mB[i] = np.zeros_like(b[i])\n",
        "                        vB[i] = np.zeros_like(b[i])\n",
        "                    grad_W = VW[i] + weight_decay * W[i]\n",
        "                    grad_b = Vb[i]\n",
        "                    mW[i] = beta1 * mW[i] + (1 - beta1) * grad_W\n",
        "                    mB[i] = beta1 * mB[i] + (1 - beta1) * grad_b\n",
        "                    vW[i] = beta2 * vW[i] + (1 - beta2) * np.square(grad_W)\n",
        "                    vB[i] = beta2 * vB[i] + (1 - beta2) * np.square(grad_b)\n",
        "                    mW_hat = mW[i] / (1 - beta1**t)\n",
        "                    mB_hat = mB[i] / (1 - beta1**t)\n",
        "                    vW_hat = vW[i] / (1 - beta2**t)\n",
        "                    vB_hat = vB[i] / (1 - beta2**t)\n",
        "                    nadam_update_W = (beta1 * mW_hat) + ((1 - beta1) * grad_W) / (1 - beta1**t)\n",
        "                    nadam_update_B = (beta1 * mB_hat) + ((1 - beta1) * grad_b) / (1 - beta1**t)\n",
        "                    W[i] -= learning_rate * nadam_update_W / (np.sqrt(vW_hat) + epsilon)\n",
        "                    b[i] -= learning_rate * nadam_update_B / (np.sqrt(vB_hat) + epsilon)\n",
        "                VW = {}\n",
        "                Vb = {}\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": total_loss})\n",
        "        evaluate_performance(x_test, y_test, x_train, y_train, W, b,\n",
        "                             hiddenlayers, hiddennodes, input_nodes, output_nodes, activation_func, loss_func)"
      ],
      "metadata": {
        "id": "noyOgtEhYj49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "vNvCLrDZw1iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                W, b, a, h, lr, epochs, batch, n_train, opt, act_func, loss_func, init_method,\n",
        "                weight_decay, momentum, beta1, beta2, epsilon, beta):\n",
        "    # Initialize weights and biases.\n",
        "    init(W, b, input_nodes, hidden_layers, hidden_nodes, output_nodes, init_method)\n",
        "\n",
        "    if opt == \"sgd\":\n",
        "        SGD(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "            W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay)\n",
        "    elif opt == \"momentum\":\n",
        "        momentum_gd(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                    W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, momentum)\n",
        "    elif opt == \"nag\":\n",
        "        nesterov_gd(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                     W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, momentum)\n",
        "    elif opt == \"rmsprop\":\n",
        "        rmsprop(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, beta, epsilon)\n",
        "    elif opt == \"adam\":\n",
        "        adam(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "             W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, beta1, beta2, epsilon)\n",
        "    elif opt == \"nadam\":\n",
        "        nadam(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "              W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, beta1, beta2, epsilon)\n",
        "    else:\n",
        "        raise ValueError(\"Optimizer option not recognized.\")\n",
        "\n",
        "    # Evaluate final performance after training.\n",
        "    final_train_acc, final_val_acc, final_test_acc = evaluate_performance(\n",
        "        x_test, y_test, x_train, y_train, W, b,\n",
        "        hidden_layers, hidden_nodes, input_nodes, output_nodes, act_func, loss_func\n",
        "    )\n",
        "    return final_train_acc, final_val_acc, final_test_acc"
      ],
      "metadata": {
        "id": "z2lqXVuyw5xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweep Configuration"
      ],
      "metadata": {
        "id": "ZMZgE58Yw-4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sweep config for wandb plotting\n",
        "sweep_config = {\n",
        "    'name'  : \"MustaqAhamed\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'hiddenlayers': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'num_epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'hiddennodes': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'initializer': {\n",
        "            'values': [\"random\", \"Xavier\"]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'opt': {\n",
        "            'values': [\"sgd\", \"mbgd\", \"nesterov\",\"rmsprop\", \"adam\", \"nadam\"]\n",
        "        },\n",
        "        'activation_func': {\n",
        "            'values': [\"sigmoid\",\"tanh\",\"relu\"]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005, 0.5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, project=\"DA6401-Assignment1\")"
      ],
      "metadata": {
        "id": "QdxgSncixCPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Network and Running Experiment"
      ],
      "metadata": {
        "id": "Y-mgLOS6xI7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing and setting training size\n",
        "num_classes = 10\n",
        "input_size = 784\n",
        "n_train_samples = 54000\n",
        "\n",
        "# Initialize parameter dictionaries\n",
        "loss_mode = \"cross_entropy\"\n",
        "weights = {}\n",
        "biases = {}\n",
        "h = {}\n",
        "a = {}\n",
        "\n",
        "def run_experiment():\n",
        "    run = wandb.init(config=sweep_config)\n",
        "    cfg = run.config\n",
        "    run.name = \"epochs {} hidden_layers {} hidden_size {} learning_rate {} opt {} batch_size {} init {} activation {} weight_decay {}\".format(\n",
        "        cfg.num_epochs, cfg.hiddenlayers, cfg.hiddennodes, cfg.learning_rate, cfg.opt, cfg.batch_size, cfg.initializer, cfg.activation_func, cfg.weight_decay\n",
        "    )\n",
        "    train_model(\n",
        "        x_train, y_train,\n",
        "        cfg.hiddenlayers, cfg.hiddennodes,\n",
        "        input_size, num_classes,\n",
        "        weights, biases, a, h,\n",
        "        cfg.learning_rate, cfg.num_epochs, cfg.batch_size,\n",
        "        n_train_samples, cfg.opt,\n",
        "        cfg.activation_func, loss_mode, cfg.initializer, cfg.weight_decay\n",
        "    )\n",
        "\n",
        "wandb.agent(\"9orvwc9k\", run_experiment,project=\"DA6401-Assignment1\" ,  count=30 )\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Q1ACKvyOxQMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}