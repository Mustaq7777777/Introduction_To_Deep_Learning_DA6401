{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustaq7777777/Introduction_To_Deep_Learning_DA6401/blob/main/Assignment1/DeepLearning_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np9y5M3mmoPL"
      },
      "outputs": [],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 1"
      ],
      "metadata": {
        "id": "ZEMlJmiIHUGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "#There are 10 class labels present in mnist data set.\n",
        "class_labels = [\n",
        "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
        "]\n",
        "\n",
        "\n",
        "wandb.init(project=\"DA6401-Assignment1\", name=\"run1\", reinit=True)\n",
        "\n",
        "#Loading data from the fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#storing images in Image_set\n",
        "images_to_log = []\n",
        "for class_index in range(10):\n",
        "\n",
        "    #Find the instance of each class and append it .\n",
        "    idx = np.where(train_labels == class_index)[0][0]\n",
        "\n",
        "\n",
        "    image = train_images[idx]\n",
        "    wandb_image = wandb.Image(image, caption=class_labels[class_index])\n",
        "    images_to_log.append(wandb_image)\n",
        "\n",
        "# Log the image to WandB\n",
        "wandb.log({\"Question1 Sample images for each class\": images_to_log})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "bU_YG1OUkeXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "Svl549ZQNN7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "eXRsP47Cv3QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)/256\n",
        "x_test = x_test.reshape(10000, 784)/256\n",
        "\n",
        "y_train = y_train.reshape(60000, 1)\n",
        "y_test = y_test.reshape(10000, 1)"
      ],
      "metadata": {
        "id": "pJ5i9fuev6CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions and their derivatives"
      ],
      "metadata": {
        "id": "n2kZHn46xC4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for the output layer\n",
        "def softmax(v):\n",
        "    exp_vector = np.exp(v - np.max(v))\n",
        "    return exp_vector / np.sum(exp_vector)\n",
        "\n",
        "# Sigmoid activation function.\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function.\n",
        "def d_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Hyperbolic tangent activation function.\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Derivative of the tanh function.\n",
        "def d_tanh(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "# Rectified Linear Unit (ReLU) activation function.\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivative of the ReLU function.\n",
        "def d_relu(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Function to create a one-hot encoded vector.\n",
        "def e(l, length):\n",
        "    y = np.zeros([length, 1])\n",
        "    y[int(l)] = 1\n",
        "    return y"
      ],
      "metadata": {
        "id": "4EKnZQSGLG1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization Of Weights and Biases"
      ],
      "metadata": {
        "id": "il0WyZkrHLyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def init(W, b, input_nodes, hiddenlayers, hiddennodes, output_nodes, initializer):\n",
        "    # Set a seed for reproducibility.\n",
        "    np.random.seed(1)\n",
        "\n",
        "    if initializer == \"Xavier\":\n",
        "        # Xavier initialization for the first hidden layer (input to hidden).\n",
        "        W[1] = np.random.normal(0.0, np.sqrt(1.0 / input_nodes), (hiddennodes, input_nodes))\n",
        "        b[1] = np.zeros((hiddennodes, 1))\n",
        "        # Xavier initialization for subsequent hidden layers.\n",
        "        for i in range(2, hiddenlayers + 1):\n",
        "            W[i] = np.random.normal(0.0, np.sqrt(1.0 / hiddennodes), (hiddennodes, hiddennodes))\n",
        "            b[i] = np.zeros((hiddennodes, 1))\n",
        "        # Xavier initialization for the output layer (hidden to output).\n",
        "        W[hiddenlayers + 1] = np.random.normal(0.0, np.sqrt(1.0 / hiddennodes), (output_nodes, hiddennodes))\n",
        "        b[hiddenlayers + 1] = np.zeros((output_nodes, 1))\n",
        "    elif initializer == \"random\":\n",
        "        # Random initialization for the first hidden layer.\n",
        "        W[1] = np.random.rand(hiddennodes, input_nodes) - 0.5\n",
        "        b[1] = np.zeros((hiddennodes, 1))\n",
        "        # Random initialization for subsequent hidden layers.\n",
        "        for i in range(2, hiddenlayers + 1):\n",
        "            W[i] = np.random.rand(hiddennodes, hiddennodes) - 0.5\n",
        "            b[i] = np.zeros((hiddennodes, 1))\n",
        "        # Random initialization for the output layer.\n",
        "        W[hiddenlayers + 1] = np.random.rand(output_nodes, hiddennodes) - 0.5\n",
        "        b[hiddenlayers + 1] = np.zeros((output_nodes, 1))\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "ykuw6EsQN1Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation"
      ],
      "metadata": {
        "id": "pyTeztydTGPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applies the selected activation function to the input.\n",
        "def g(z, act_func):\n",
        "    if act_func == \"sigmoid\":\n",
        "        return sigmoid(z)\n",
        "    elif act_func == \"tanh\":\n",
        "        return tanh(z)\n",
        "    elif act_func == \"relu\":\n",
        "        return relu(z)\n",
        "\n",
        "# Applies the derivative of the selected activation function.\n",
        "def g_derivative(z, act_func):\n",
        "    if act_func == \"sigmoid\":\n",
        "        return d_sigmoid(z)\n",
        "    elif act_func == \"tanh\":\n",
        "        return d_tanh(z)\n",
        "    elif act_func == \"relu\":\n",
        "        return d_relu(z)\n",
        "\n",
        "# Computes the output layer activation using softmax.\n",
        "def output_activation(z, act_func):\n",
        "    if act_func == \"softmax\":\n",
        "        return softmax(z)\n",
        "\n",
        "# Performs forward propagation for a single training example.\n",
        "def forward_propagation(x_data, hidden_layers, hidden_nodes, input_nodes, output_nodes, W, b, a, h, L, act_func):\n",
        "    # Set input layer activation by reshaping the L-th training example.\n",
        "    h[0] = x_data[L].reshape(-1, 1)\n",
        "    # Forward pass through all hidden layers.\n",
        "    layer = 1\n",
        "    while layer <= hidden_layers:\n",
        "        # Compute linear combination: weighted sum + bias.\n",
        "        a[layer] = np.dot(W[layer], h[layer - 1]) + b[layer]\n",
        "        # Apply activation function.\n",
        "        h[layer] = g(a[layer], act_func)\n",
        "        layer += 1\n",
        "    # Compute output layer activation.\n",
        "    a[hidden_layers + 1] = np.dot(W[hidden_layers + 1], h[hidden_layers]) + b[hidden_layers + 1]\n",
        "    ycap = output_activation(a[hidden_layers + 1], \"softmax\")\n",
        "    return ycap, a, h\n"
      ],
      "metadata": {
        "id": "VlVPL6vpTJ4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back Propagation"
      ],
      "metadata": {
        "id": "ir9Khj8CY3u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                     W, b, a, h, gradient_weights, gradient_biases, sample_idx, yhat, activation_func, loss_func):\n",
        "\n",
        "    output_layer = hiddenlayers + 1\n",
        "    delta = {}\n",
        "\n",
        "    # Compute the error (delta) for the output layer based on the chosen loss function.\n",
        "    if loss_func == \"mean_squared_error\":\n",
        "        target = e(y_train[sample_idx].item(), output_nodes)\n",
        "        # Calculate Jacobian for the softmax output.\n",
        "        jacobian = np.diagflat(yhat) - np.dot(yhat, yhat.T)\n",
        "        # Chain rule application and scaling by the number of output nodes.\n",
        "        delta[output_layer] = np.dot(jacobian, (yhat - target)) / output_nodes\n",
        "    elif loss_func == \"cross_entropy\":\n",
        "        delta[output_layer] = yhat - e(y_train[sample_idx].item(), output_nodes)\n",
        "\n",
        "\n",
        "\n",
        "    # Propagate the error backward through the network.\n",
        "    k = output_layer\n",
        "    while k > 0:\n",
        "        # Compute gradients for weights and biases.\n",
        "        grad_W = np.dot(delta[k], h[k-1].T)\n",
        "        grad_b = delta[k]\n",
        "        # Accumulate gradients in gradient_weights and gradient_biases dictionaries.\n",
        "        if k in gradient_weights:\n",
        "            gradient_weights[k] += grad_W\n",
        "            gradient_biases[k] += grad_b\n",
        "        else:\n",
        "            gradient_weights[k] = grad_W\n",
        "            gradient_biases[k] = grad_b\n",
        "        # Backpropagate the error to the previous layer.\n",
        "        if k > 1:\n",
        "            prev_error = np.dot(W[k].T, delta[k])\n",
        "            delta[k-1] = np.multiply(prev_error, g_derivative(a[k-1], activation_func).reshape(-1, 1))\n",
        "        k -= 1\n",
        "\n",
        "    return gradient_weights, gradient_biases"
      ],
      "metadata": {
        "id": "YVkIsRrwY8Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "n6Fs8ITL0kzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the loss for a single training example.\n",
        "def loss(yhat, y_train, loss_func, sample_idx, output_nodes):\n",
        "    if loss_func == \"mean_squared_error\":\n",
        "        target = e(y_train[sample_idx].item(), output_nodes)\n",
        "        return np.mean(np.square(yhat - target))\n",
        "    elif loss_func == \"cross_entropy\":\n",
        "        return -np.log(yhat[y_train[sample_idx].item()])"
      ],
      "metadata": {
        "id": "sWj5Vvyowdfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating Performance and Plotting"
      ],
      "metadata": {
        "id": "Ha3GKZ_w_Wza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data performance evaluation\n",
        "def evaluate_performance_train(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                               W, b, activation_func, loss_func):\n",
        "    # Evaluate training performance on the first 54000 samples.\n",
        "    correct_train = 0\n",
        "    total_train_loss = 0\n",
        "    # going over training examples\n",
        "    for idx in range(0, 54000):\n",
        "        yhat, _, _ = forward_propagation(x_train, hidden_layers, hidden_nodes, input_nodes,\n",
        "                                         output_nodes, W, b, {}, {}, idx, activation_func)\n",
        "        total_train_loss += loss(yhat, y_train, loss_func, idx, output_nodes)\n",
        "        pred = int(np.argmax(yhat))\n",
        "        if pred == y_train[idx]:\n",
        "            correct_train += 1\n",
        "\n",
        "    # accuracy and loss calculation\n",
        "    train_accuracy = (correct_train / 54000) * 100\n",
        "    train_loss = total_train_loss / 54000\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"train_loss\": train_loss})\n",
        "    return train_accuracy, train_loss\n",
        "\n",
        "# Validation data performance evaluation\n",
        "def evaluate_performance_validity(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                                  W, b, activation_func, loss_func):\n",
        "    # Evaluate validation performance on the last 6000 training samples.\n",
        "    correct_val = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    # going over validation dataset\n",
        "    for idx in range(54000, 60000):\n",
        "        yhat, _, _ = forward_propagation(x_train, hidden_layers, hidden_nodes, input_nodes,\n",
        "                                         output_nodes, W, b, {}, {}, idx, activation_func)\n",
        "        total_val_loss += loss(yhat, y_train, loss_func, idx, output_nodes)\n",
        "        pred = int(np.argmax(yhat))\n",
        "        if pred == y_train[idx]:\n",
        "            correct_val += 1\n",
        "\n",
        "    # accuracy and loss calculation\n",
        "    val_accuracy = (correct_val / (60000 - 54000)) * 100\n",
        "    val_loss = total_val_loss / (60000 - 54000)\n",
        "    wandb.log({\"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "    return val_accuracy, val_loss\n",
        "\n",
        "# Test data Performance Evaluation\n",
        "def evaluate_performance_test(x_test, y_test, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                              W, b, activation_func):\n",
        "    # Evaluate test accuracy.\n",
        "    correct_test = 0\n",
        "\n",
        "    # going over test dataset\n",
        "    for idx in range(len(x_test)):\n",
        "        yhat, _, _ = forward_propagation(x_test, hidden_layers, hidden_nodes, input_nodes,\n",
        "                                         output_nodes, W, b, {}, {}, idx, activation_func)\n",
        "        pred = int(np.argmax(yhat))\n",
        "        if pred == y_test[idx]:\n",
        "            correct_test += 1\n",
        "\n",
        "    # accuracy calculation\n",
        "    test_accuracy = (correct_test / len(x_test)) * 100\n",
        "    wandb.log({\"test_accuracy\": test_accuracy})\n",
        "    return test_accuracy"
      ],
      "metadata": {
        "id": "VuyDBn51_bQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "x2GrKfYcqWfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "        W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "        activation_func, loss_func, weight_decay):\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Initialize gradient accumulators.\n",
        "        gradient_weights = {}\n",
        "        gradient_biases = {}\n",
        "        # Loop over all training samples.\n",
        "        for index in range(train_set):\n",
        "            # Forward propagation for current sample.\n",
        "            yhat, a, h = forward_propagation(x_train, hiddenlayers, hiddennodes, input_nodes,\n",
        "                                             output_nodes, W, b, a, h, index, activation_func)\n",
        "            # Compute gradients via backpropagation.\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, gradient_weights, gradient_biases, index, yhat, activation_func, loss_func)\n",
        "            # Update weights in batches.\n",
        "            if index % batch_size == 0:\n",
        "                i = 1\n",
        "                while i < hiddenlayers + 2:\n",
        "                    W[i] -= learning_rate * (gradient_weights[i] + weight_decay * W[i])\n",
        "                    b[i] -= learning_rate * gradient_biases[i]\n",
        "                    i += 1\n",
        "                # Reset gradient accumulators after each batch.\n",
        "                gradient_weights, gradient_biases = {}, {}\n",
        "        # Log epoch number.\n",
        "        wandb.log({\"epoch \" : epoch})\n",
        "\n",
        "        # Evaluate and log performance on test, validation, and training sets.\n",
        "        evaluate_performance_train(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_validity(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_test(x_test, y_test, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func)\n",
        ""
      ],
      "metadata": {
        "id": "PC-1Vw7yqdru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum Based Gradient Descent"
      ],
      "metadata": {
        "id": "8JG-RSTNqjEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gd(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "                activation_func, loss_func, weight_decay, momentum):\n",
        "    # Initialize velocity terms.\n",
        "    velocity_weights = {}\n",
        "    velocity_biases = {}\n",
        "    for epoch in range(num_epochs):\n",
        "        gradient_weights = {}\n",
        "        gradient_biases = {}\n",
        "        for index in range(train_set):\n",
        "            yhat, a, h = forward_propagation(x_train, hiddenlayers, hiddennodes, input_nodes,\n",
        "                                             output_nodes, W, b, a, h, index, activation_func)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, gradient_weights, gradient_biases, index, yhat, activation_func, loss_func)\n",
        "            if index % batch_size == 0:\n",
        "                i = 1\n",
        "                while i < hiddenlayers + 2:\n",
        "                    if i not in velocity_weights:\n",
        "                        velocity_weights[i] = np.zeros_like(W[i])\n",
        "                        velocity_biases[i] = np.zeros_like(b[i])\n",
        "                    # Update velocity terms.\n",
        "                    velocity_weights[i] = momentum * velocity_weights[i] + learning_rate * (gradient_weights[i] + weight_decay * W[i])\n",
        "                    velocity_biases[i] = momentum * velocity_biases[i] + learning_rate * gradient_biases[i]\n",
        "                    # Update weights and biases using the velocity.\n",
        "                    W[i] -= velocity_weights[i]\n",
        "                    b[i] -= velocity_biases[i]\n",
        "                    i += 1\n",
        "                gradient_weights, gradient_biases = {}, {}\n",
        "\n",
        "        # Log epoch number.\n",
        "        wandb.log({\"epoch \" : epoch})\n",
        "\n",
        "        # Evaluate and log performance on test, validation, and training sets.\n",
        "        evaluate_performance_train(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_validity(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_test(x_test, y_test, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func)"
      ],
      "metadata": {
        "id": "aXa9p9Ngqoh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesterov"
      ],
      "metadata": {
        "id": "XWcqvxZ62Xno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gd(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "                activation_func, loss_func, weight_decay, momentum):\n",
        "    velocity_weights = {}\n",
        "    velocity_biases = {}\n",
        "    for epoch in range(num_epochs):\n",
        "        gradient_weights = {}\n",
        "        gradient_biases = {}\n",
        "        for index in range(train_set):\n",
        "            # Create lookahead weights and biases.\n",
        "            W_temp = {}\n",
        "            b_temp = {}\n",
        "            for i in range(1, hiddenlayers + 2):\n",
        "                if i not in velocity_weights:\n",
        "                    velocity_weights[i] = np.zeros_like(W[i])\n",
        "                    velocity_biases[i] = np.zeros_like(b[i])\n",
        "                W_temp[i] = W[i] - momentum * velocity_weights[i]\n",
        "                b_temp[i] = b[i] - momentum * velocity_biases[i]\n",
        "            # Forward pass with lookahead parameters.\n",
        "            yhat, a_temp, h_temp = forward_propagation(x_train, hiddenlayers, hiddennodes, input_nodes,\n",
        "                                                       output_nodes, W_temp, b_temp, a, h, index, activation_func)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W_temp, b_temp, a_temp, h_temp, gradient_weights, gradient_biases, index, yhat, activation_func, loss_func)\n",
        "            if index % batch_size == 0:\n",
        "                i = 1\n",
        "                while i < hiddenlayers + 2:\n",
        "                    velocity_weights[i] = momentum * velocity_weights[i] + learning_rate * (gradient_weights[i] + weight_decay * W[i])\n",
        "                    velocity_biases[i] = momentum * velocity_biases[i] + learning_rate * gradient_biases[i]\n",
        "                    W[i] -= velocity_weights[i]\n",
        "                    b[i] -= velocity_biases[i]\n",
        "                    i += 1\n",
        "                gradient_weights, gradient_biases = {}, {}\n",
        "\n",
        "        # Log epoch number.\n",
        "        wandb.log({\"epoch \" : epoch})\n",
        "\n",
        "        # Evaluate and log performance on test, validation, and training sets.\n",
        "        evaluate_performance_train(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_validity(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_test(x_test, y_test, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func)"
      ],
      "metadata": {
        "id": "It9BsNsV2aEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSprop"
      ],
      "metadata": {
        "id": "hB4pd3ecYHGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "            W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "            activation_func, loss_func, weight_decay, beta, epsilon):\n",
        "    # Initialize squared gradient accumulators.\n",
        "    squared_grad_weights = {}\n",
        "    squared_grad_biases = {}\n",
        "    for epoch in range(num_epochs):\n",
        "        gradient_weights = {}\n",
        "        gradient_biases = {}\n",
        "        for index in range(train_set):\n",
        "            yhat, a_temp, h_temp = forward_propagation(x_train, hiddenlayers, hiddennodes,\n",
        "                                                       input_nodes, output_nodes, W, b, a, h, index, activation_func)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, gradient_weights, gradient_biases, index, yhat, activation_func, loss_func)\n",
        "            if index % batch_size == 0:\n",
        "                i = 1\n",
        "                while i < hiddenlayers + 2:\n",
        "                    if i not in squared_grad_weights:\n",
        "                        squared_grad_weights[i] = np.zeros_like(W[i])\n",
        "                        squared_grad_biases[i] = np.zeros_like(b[i])\n",
        "                    grad_W = gradient_weights[i] + weight_decay * W[i]\n",
        "                    grad_b = gradient_biases[i]\n",
        "                    # Update running averages of squared gradients.\n",
        "                    squared_grad_weights[i] = beta * squared_grad_weights[i] + (1 - beta) * np.square(grad_W)\n",
        "                    squared_grad_biases[i] = beta * squared_grad_biases[i] + (1 - beta) * np.square(grad_b)\n",
        "                    # Update weights and biases.\n",
        "                    W[i] -= learning_rate * grad_W / (np.sqrt(squared_grad_weights[i]) + epsilon)\n",
        "                    b[i] -= learning_rate * grad_b / (np.sqrt(squared_grad_biases[i]) + epsilon)\n",
        "                    i += 1\n",
        "                gradient_weights, gradient_biases = {}, {}\n",
        "\n",
        "        # Log epoch number.\n",
        "        wandb.log({\"epoch \" : epoch})\n",
        "\n",
        "        # Evaluate and log performance on test, validation, and training sets.\n",
        "        evaluate_performance_train(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_validity(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_test(x_test, y_test, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func)"
      ],
      "metadata": {
        "id": "s6GLWDRYYS63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam"
      ],
      "metadata": {
        "id": "Gi8htOi-YZUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "         W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "         activation_func, loss_func, weight_decay, beta1, beta2, epsilon):\n",
        "    # Initialize first moment (m) and second moment (v) estimates.\n",
        "    first_momentum_weights = {}\n",
        "    second_momentum_weights = {}\n",
        "    first_momentum_biases = {}\n",
        "    second_momentum_biases = {}\n",
        "    t = 0  # Time step counter.\n",
        "    for epoch in range(num_epochs):\n",
        "        gradient_weights = {}\n",
        "        gradient_biases = {}\n",
        "        for index in range(train_set):\n",
        "            yhat, a_temp, h_temp = forward_propagation(x_train, hiddenlayers, hiddennodes,\n",
        "                                                       input_nodes, output_nodes, W, b, a, h, index, activation_func)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, gradient_weights, gradient_biases, index, yhat, activation_func, loss_func)\n",
        "            if index % batch_size == 0:\n",
        "                t += 1  # Increment time step.\n",
        "                i = 1\n",
        "                while i < hiddenlayers + 2:\n",
        "                    if i not in first_momentum_weights:\n",
        "                        first_momentum_weights[i] = np.zeros_like(W[i])\n",
        "                        second_momentum_weights[i] = np.zeros_like(W[i])\n",
        "                        first_momentum_biases[i] = np.zeros_like(b[i])\n",
        "                        second_momentum_biases[i] = np.zeros_like(b[i])\n",
        "                    grad_W = gradient_weights[i] + weight_decay * W[i]\n",
        "                    grad_b = gradient_biases[i]\n",
        "                    # Update biased first moment estimate.\n",
        "                    first_momentum_weights[i] = beta1 * first_momentum_weights[i] + (1 - beta1) * grad_W\n",
        "                    first_momentum_biases[i] = beta1 * first_momentum_biases[i] + (1 - beta1) * grad_b\n",
        "                    # Update biased second raw moment estimate.\n",
        "                    second_momentum_weights[i] = beta2 * second_momentum_weights[i] + (1 - beta2) * np.square(grad_W)\n",
        "                    second_momentum_biases[i] = beta2 * second_momentum_biases[i] + (1 - beta2) * np.square(grad_b)\n",
        "                    # Compute bias-corrected first moment estimates.\n",
        "                    mW_hat = first_momentum_weights[i] / (1 - beta1**t)\n",
        "                    mB_hat = first_momentum_biases[i] / (1 - beta1**t)\n",
        "                    # Compute bias-corrected second moment estimates.\n",
        "                    vW_hat = second_momentum_weights[i] / (1 - beta2**t)\n",
        "                    vB_hat = second_momentum_biases[i] / (1 - beta2**t)\n",
        "                    # Update weights and biases.\n",
        "                    W[i] -= learning_rate * mW_hat / (np.sqrt(vW_hat) + epsilon)\n",
        "                    b[i] -= learning_rate * mB_hat / (np.sqrt(vB_hat) + epsilon)\n",
        "                    i += 1\n",
        "                gradient_weights, gradient_biases = {}, {}\n",
        "\n",
        "        # Log epoch number.\n",
        "        wandb.log({\"epoch \" : epoch})\n",
        "\n",
        "        # Evaluate and log performance on test, validation, and training sets.\n",
        "        evaluate_performance_train(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_validity(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_test(x_test, y_test, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func)"
      ],
      "metadata": {
        "id": "-0hYG1hQYbE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nadam"
      ],
      "metadata": {
        "id": "3ZuQq_lQYiXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "          W, b, a, h, learning_rate, num_epochs, batch_size, train_set,\n",
        "          activation_func, loss_func, weight_decay, beta1, beta2, epsilon):\n",
        "    first_momentum_weights = {}\n",
        "    second_momentum_weights = {}\n",
        "    first_momentum_biases = {}\n",
        "    second_momentum_biases = {}\n",
        "    t = 0  # Time step counter.\n",
        "    for epoch in range(num_epochs):\n",
        "        gradient_weights = {}\n",
        "        gradient_biases = {}\n",
        "        for index in range(train_set):\n",
        "            yhat, a_temp, h_temp = forward_propagation(x_train, hiddenlayers, hiddennodes,\n",
        "                                                       input_nodes, output_nodes, W, b, a, h, index, activation_func)\n",
        "            back_propagation(y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes,\n",
        "                             W, b, a, h, gradient_weights, gradient_biases, index, yhat, activation_func, loss_func)\n",
        "            if index % batch_size == 0:\n",
        "                t += 1  # Increment time step.\n",
        "                i = 1\n",
        "                while i < hiddenlayers + 2:\n",
        "                    if i not in first_momentum_weights:\n",
        "                        first_momentum_weights[i] = np.zeros_like(W[i])\n",
        "                        second_momentum_weights[i] = np.zeros_like(W[i])\n",
        "                        first_momentum_biases[i] = np.zeros_like(b[i])\n",
        "                        second_momentum_biases[i] = np.zeros_like(b[i])\n",
        "                    grad_W = gradient_weights[i] + weight_decay * W[i]\n",
        "                    grad_b = gradient_biases[i]\n",
        "                    # Update biased first moment estimate.\n",
        "                    first_momentum_weights[i] = beta1 * first_momentum_weights[i] + (1 - beta1) * grad_W\n",
        "                    first_momentum_biases[i] = beta1 * first_momentum_biases[i] + (1 - beta1) * grad_b\n",
        "                    # Update biased second raw moment estimate.\n",
        "                    second_momentum_weights[i] = beta2 * second_momentum_weights[i] + (1 - beta2) * np.square(grad_W)\n",
        "                    second_momentum_biases[i] = beta2 * second_momentum_biases[i] + (1 - beta2) * np.square(grad_b)\n",
        "                    # Compute bias-corrected estimates.\n",
        "                    mW_hat = first_momentum_weights[i] / (1 - beta1**t)\n",
        "                    mB_hat = first_momentum_biases[i] / (1 - beta1**t)\n",
        "                    vW_hat = second_momentum_weights[i] / (1 - beta2**t)\n",
        "                    vB_hat = second_momentum_biases[i] / (1 - beta2**t)\n",
        "                    # Nadam update rule for weights and biases.\n",
        "                    nadam_update_W = (beta1 * mW_hat) + ((1 - beta1) * grad_W) / (1 - beta1**t)\n",
        "                    nadam_update_B = (beta1 * mB_hat) + ((1 - beta1) * grad_b) / (1 - beta1**t)\n",
        "                    W[i] -= learning_rate * nadam_update_W / (np.sqrt(vW_hat) + epsilon)\n",
        "                    b[i] -= learning_rate * nadam_update_B / (np.sqrt(vB_hat) + epsilon)\n",
        "                    i += 1\n",
        "                gradient_weights, gradient_biases = {}, {}\n",
        "\n",
        "        # Log epoch number.\n",
        "        wandb.log({\"epoch \" : epoch})\n",
        "\n",
        "        # Evaluate and log performance on test, validation, and training sets.\n",
        "        evaluate_performance_train(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_validity(x_train, y_train, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func, loss_func)\n",
        "        evaluate_performance_test(x_test, y_test, hiddenlayers, hiddennodes, input_nodes, output_nodes, W, b, activation_func)"
      ],
      "metadata": {
        "id": "noyOgtEhYj49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "vNvCLrDZw1iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                W, b, a, h, lr, epochs, batch, n_train, opt, act_func, loss_func, init_method,\n",
        "                weight_decay, momentum, beta1, beta2, epsilon, beta):\n",
        "    # Initialize weights and biases using the specified method.\n",
        "    init(W, b, input_nodes, hidden_layers, hidden_nodes, output_nodes, init_method)\n",
        "\n",
        "    # Choose the optimizer based on user input.\n",
        "    if opt == \"nadam\":\n",
        "        nadam(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "              W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, beta1, beta2, epsilon)\n",
        "    elif opt == \"adam\":\n",
        "        adam(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "             W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, beta1, beta2, epsilon)\n",
        "    elif opt == \"rmsprop\":\n",
        "        rmsprop(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, beta, epsilon)\n",
        "    elif opt == \"nag\":\n",
        "        nesterov_gd(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                     W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, momentum)\n",
        "    elif opt == \"momentum\":\n",
        "        momentum_gd(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "                    W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay, momentum)\n",
        "    elif opt == \"sgd\":\n",
        "        SGD(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes,\n",
        "            W, b, a, h, lr, epochs, batch, n_train, act_func, loss_func, weight_decay)\n",
        "    else:\n",
        "        raise ValueError(\"Optimizer option not recognized.\")\n",
        "\n",
        "    # Evaluate and return the final performance after training.\n",
        "    train_accuracy, train_loss = evaluate_performance_train(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes, W, b, act_func, loss_func)\n",
        "    val_accuracy, val_loss = evaluate_performance_validity(x_train, y_train, hidden_layers, hidden_nodes, input_nodes, output_nodes, W, b, act_func, loss_func)\n",
        "    test_accuracy = evaluate_performance_test(x_test, y_test, hidden_layers, hidden_nodes, input_nodes, output_nodes, W, b, act_func)\n",
        "    return train_accuracy, val_accuracy, test_accuracy\n"
      ],
      "metadata": {
        "id": "z2lqXVuyw5xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweep Configuration"
      ],
      "metadata": {
        "id": "ZMZgE58Yw-4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sweep config for wandb plotting\n",
        "sweep_config = {\n",
        "    'name'  : \"MustaqAhamed\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'hiddenlayers': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'num_epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'hiddennodes': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'initializer': {\n",
        "            'values': [\"random\", \"Xavier\"]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'opt': {\n",
        "            'values': [\"sgd\", \"mbgd\", \"nesterov\",\"rmsprop\", \"adam\", \"nadam\"]\n",
        "        },\n",
        "        'activation_func': {\n",
        "            'values': [\"sigmoid\",\"tanh\",\"relu\"]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005, 0.5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DA6401-Assignment1\")"
      ],
      "metadata": {
        "id": "QdxgSncixCPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Network and Running Experiment"
      ],
      "metadata": {
        "id": "Y-mgLOS6xI7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing and setting training size\n",
        "num_classes = 10\n",
        "input_size = 784\n",
        "n_train_samples = 54000\n",
        "\n",
        "# Initialize parameter dictionaries\n",
        "loss_mode = \"cross_entropy\"\n",
        "weights = {}\n",
        "biases = {}\n",
        "h = {}\n",
        "a = {}\n",
        "\n",
        "def run_experiment():\n",
        "    run = wandb.init(config=sweep_config)\n",
        "    cfg = run.config\n",
        "    run.name = \"epochs {} hidden_layers {} hidden_size {} learning_rate {} opt {} batch_size {} init {} activation {} weight_decay {}\".format(\n",
        "        cfg.num_epochs, cfg.hiddenlayers, cfg.hiddennodes, cfg.learning_rate, cfg.opt, cfg.batch_size, cfg.initializer, cfg.activation_func, cfg.weight_decay\n",
        "    )\n",
        "    train_model(\n",
        "        x_train, y_train,\n",
        "        cfg.hiddenlayers, cfg.hiddennodes,\n",
        "        input_size, num_classes,\n",
        "        weights, biases, a, h,\n",
        "        cfg.learning_rate, cfg.num_epochs, cfg.batch_size,\n",
        "        n_train_samples, cfg.opt,\n",
        "        cfg.activation_func, loss_mode, cfg.initializer, cfg.weight_decay, 0.9, 0.999, 1e-8, 0.9\n",
        "    )\n",
        "\n",
        "wandb.agent(sweep_id, run_experiment,project=\"DA6401-Assignment1\" ,  count=30 )\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Q1ACKvyOxQMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Model Finding and Confusion Matrix generation for Test data"
      ],
      "metadata": {
        "id": "eLhzOWxqH5Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Best hyperparameter configuration (using adam as best)\n",
        "config = {\n",
        "    \"hiddenlayers\": 5,\n",
        "    \"hiddennodes\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"num_epochs\": 10,\n",
        "    \"batch_size\": 64,\n",
        "    \"opt\": \"adam\",         # Best performance optimizer\n",
        "    \"initializer\": \"Xavier\",\n",
        "    \"activation_func\": \"relu\",\n",
        "    \"weight_decay\": 0\n",
        "}\n",
        "\n",
        "# Create a unique run name (if needed, append a timestamp or random string)\n",
        "run_name = \"Best Model: epochs {} hidden_layers {} hidden_size {} learning_rate {} opt {} batch_size {} init {} activation {} weight_decay {}\".format(\n",
        "    config[\"num_epochs\"],\n",
        "    config[\"hiddenlayers\"],\n",
        "    config[\"hiddennodes\"],\n",
        "    config[\"learning_rate\"],\n",
        "    config[\"opt\"],\n",
        "    config[\"batch_size\"],\n",
        "    config[\"initializer\"],\n",
        "    config[\"activation_func\"],\n",
        "    config[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "# Initialize a WandB run using the config dictionary (not sweep_config)\n",
        "run = wandb.init(project=\"DA6401-Assignment1\", config=config, reinit=True, name=run_name)\n",
        "cfg = run.config\n",
        "print(\"Run config:\", cfg)\n",
        "print(\"Run name set to:\", run.name)\n",
        "\n",
        "# Initialize model parameter dictionaries\n",
        "weights = {}\n",
        "biases = {}\n",
        "a = {}\n",
        "h = {}\n",
        "\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "n_train_samples = 54000\n",
        "loss_mode = \"cross_entropy\"\n",
        "\n",
        "# Train the model using the best parameters\n",
        "train_model(\n",
        "    x_train, y_train,\n",
        "    config[\"hiddenlayers\"], config[\"hiddennodes\"],\n",
        "    input_size, num_classes,\n",
        "    weights, biases, a, h,\n",
        "    config[\"learning_rate\"], config[\"num_epochs\"],\n",
        "    config[\"batch_size\"], n_train_samples,\n",
        "    config[\"opt\"], config[\"activation_func\"],\n",
        "    loss_mode, config[\"initializer\"],\n",
        "    config[\"weight_decay\"], 0.9, 0.999, 1e-8, 0.9\n",
        ")\n",
        "\n",
        "# Function to generate and log the confusion matrix using the trained model\n",
        "def generate_confusion_matrix(x_test, y_test, weights, biases, hidden_layers, hidden_nodes, input_nodes, output_nodes, activation_func, class_labels):\n",
        "    preds = []\n",
        "    for idx in range(len(x_test)):\n",
        "        yhat, _, _ = forward_propagation(x_test, hidden_layers, hidden_nodes, input_nodes, output_nodes, weights, biases, {}, {}, idx, activation_func)\n",
        "        preds.append(int(np.argmax(yhat)))\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix for Best Model\")\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
        "    plt.show()\n",
        "\n",
        "# Generate and log the confusion matrix using the test data\n",
        "generate_confusion_matrix(\n",
        "    x_test, y_test,\n",
        "    weights, biases,\n",
        "    config[\"hiddenlayers\"], config[\"hiddennodes\"],\n",
        "    input_size, num_classes,\n",
        "    config[\"activation_func\"],\n",
        "    class_labels\n",
        ")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "DHqhGarwIAcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEST MODEL FOR THE MNIST DATASETS"
      ],
      "metadata": {
        "id": "I_nLhyc5eA2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess Data\n",
        "x_train = x_train.reshape(60000, 784) / 256\n",
        "x_test = x_test.reshape(10000, 784) / 256\n",
        "y_train = y_train.reshape(60000, 1)\n",
        "y_test = y_test.reshape(10000, 1)\n",
        "\n",
        "# Define class labels for MNIST digits (0-9)\n",
        "class_labels = [str(i) for i in range(10)]\n",
        "\n",
        "# # Best configurations chosen from given options\n",
        "best_configs_mnist = [\n",
        "    {\n",
        "        \"hiddenlayers\": 5,\n",
        "        \"hiddennodes\": 64,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"num_epochs\": 10,\n",
        "        \"batch_size\": 64,\n",
        "        \"opt\": \"adam\",\n",
        "        \"initializer\": \"Xavier\",\n",
        "        \"activation_func\": \"relu\",\n",
        "        \"weight_decay\": 0.0005\n",
        "    },\n",
        "    {\n",
        "        \"hiddenlayers\": 4,\n",
        "        \"hiddennodes\": 64,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"num_epochs\": 10,\n",
        "        \"batch_size\": 32,\n",
        "        \"opt\": \"nadam\",\n",
        "        \"initializer\": \"Xavier\",\n",
        "        \"activation_func\": \"relu\",\n",
        "        \"weight_decay\": 0.0005\n",
        "    },\n",
        "    {\n",
        "        \"hiddenlayers\": 3,\n",
        "        \"hiddennodes\": 128,\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"num_epochs\": 10,\n",
        "        \"batch_size\": 16,\n",
        "        \"opt\": \"rmsprop\",\n",
        "        \"initializer\": \"Xavier\",\n",
        "        \"activation_func\": \"tanh\",\n",
        "        \"weight_decay\": 0.0005\n",
        "    }\n",
        "]\n",
        "\n",
        "# Define a sweep configuration that selects one of the three configurations via a config index.\n",
        "sweep_config = {\n",
        "    \"name\": \"MNIST_Sweep\",\n",
        "    \"method\": \"grid\",\n",
        "    \"parameters\": {\n",
        "        \"config_index\": {\"values\": [0, 1, 2]}\n",
        "    },\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_acc\",\n",
        "        \"goal\": \"maximize\"\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"MNIST_Experiments\")\n",
        "\n",
        "def run_experiment():\n",
        "    # Initialize a wandb run; the sweep will set config.config_index\n",
        "    run = wandb.init()\n",
        "    cfg = run.config\n",
        "    idx = cfg.config_index\n",
        "\n",
        "    # Pick the best configuration corresponding to this index\n",
        "    config = best_configs_mnist[idx]\n",
        "    # Update the run config with the chosen configuration values\n",
        "    run.config.update(config, allow_val_change=True)\n",
        "\n",
        "    # Set run name accordingly\n",
        "    run.name = \"MNIST: epochs_{}_hidden_layers_{}_hidden_size_{}_learning_rate_{}_opt_{}_batch_size_{}_init_{}_activation_{}_weight_decay_{}\".format(\n",
        "        config[\"num_epochs\"], config[\"hiddenlayers\"], config[\"hiddennodes\"], config[\"learning_rate\"],\n",
        "        config[\"opt\"], config[\"batch_size\"], config[\"initializer\"], config[\"activation_func\"], config[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    # Initialize model parameters\n",
        "    weights = {}\n",
        "    biases = {}\n",
        "    a = {}\n",
        "    h = {}\n",
        "    input_size = 784\n",
        "    num_classes = 10\n",
        "    n_train_samples = 54000\n",
        "    loss_mode = \"cross_entropy\"\n",
        "\n",
        "    # Call the training function (ensure your train_model function accepts weight_decay as the last parameter)\n",
        "    train_model(\n",
        "        x_train, y_train,\n",
        "        config[\"hiddenlayers\"], config[\"hiddennodes\"],\n",
        "        input_size, num_classes,\n",
        "        weights, biases, a, h,\n",
        "        config[\"learning_rate\"], config[\"num_epochs\"],\n",
        "        config[\"batch_size\"], n_train_samples,\n",
        "        config[\"opt\"], config[\"activation_func\"],\n",
        "        loss_mode, config[\"initializer\"],\n",
        "        config[\"weight_decay\"], 0.9, 0.999, 1e-8, 0.9\n",
        "    )\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "wandb.agent(sweep_id, run_experiment)\n"
      ],
      "metadata": {
        "id": "2wTxsExFeE-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}