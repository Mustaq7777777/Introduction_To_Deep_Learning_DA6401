{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzebEDQF3ETQ6MTZvsDeb4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustaq7777777/Introduction_To_Deep_Learning_DA6401/blob/main/Assignment1/DeepLearning_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np9y5M3mmoPL"
      },
      "outputs": [],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 1"
      ],
      "metadata": {
        "id": "ZEMlJmiIHUGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "#There are 10 class labels present in mnist data set.\n",
        "class_labels = [\n",
        "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
        "]\n",
        "\n",
        "\n",
        "wandb.init(project=\"DA6401-Assignment1\", name=\"run1\", reinit=True)\n",
        "\n",
        "#Loading data from the fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#storing images in Image_set\n",
        "images_to_log = []\n",
        "for class_index in range(10):\n",
        "\n",
        "    #Find the instance of each class and append it .\n",
        "    idx = np.where(train_labels == class_index)[0][0]\n",
        "\n",
        "\n",
        "    image = train_images[idx]\n",
        "    wandb_image = wandb.Image(image, caption=class_labels[class_index])\n",
        "    images_to_log.append(wandb_image)\n",
        "\n",
        "# Log the image to WandB\n",
        "wandb.log({\"Question1 Sample images for each class\": images_to_log})\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "bU_YG1OUkeXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "Svl549ZQNN7h"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions and their derivatives"
      ],
      "metadata": {
        "id": "n2kZHn46xC4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def softmax(v):\n",
        "    exp_vector = np.exp(v - np.max(v))\n",
        "    return exp_vector / np.sum(exp_vector)\n",
        "\n",
        "# sigmoid\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# tanh\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "# relu\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def d_relu(x):\n",
        "    if x > 0:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "# one hot vector\n",
        "def e(l, len):\n",
        "    y = np.zeros([len, 1])\n",
        "    y[l] = 1\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "4EKnZQSGLG1T"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization Of Weights and Biases"
      ],
      "metadata": {
        "id": "il0WyZkrHLyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def init(W, b, input_nodes, hiddenlayers, hiddennodes, output_nodes, initializer):\n",
        "\n",
        "    if initializer == \"Xavier\":\n",
        "        # Xavier initialization for the first hidden layer (input to first hidden layer)\n",
        "        W[1] = np.random.normal(0.0, np.sqrt(1.0 / input_nodes), (hiddennodes, input_nodes))\n",
        "        b[1] = np.zeros((hiddennodes, 1))\n",
        "\n",
        "        # Xavier initialization for subsequent hidden layers\n",
        "        for i in range(2, hiddenlayers + 1):\n",
        "            W[i] = np.random.normal(0.0, np.sqrt(1.0 / hiddennodes), (hiddennodes, hiddennodes))\n",
        "            b[i] = np.zeros((hiddennodes, 1))\n",
        "\n",
        "        # Xavier initialization for the output layer (from last hidden layer to output)\n",
        "        W[hiddenlayers + 1] = np.random.normal(0.0, np.sqrt(1.0 / hiddennodes), (output_nodes, hiddennodes))\n",
        "        b[hiddenlayers + 1] = np.zeros((output_nodes, 1))\n",
        "\n",
        "    elif initializer == \"random\":\n",
        "        # Random initialization for the first hidden layer (input to first hidden layer)\n",
        "        W[1] = np.random.rand(hiddennodes, input_nodes) - 0.5\n",
        "        b[1] = np.zeros((hiddennodes, 1))\n",
        "\n",
        "        # Random initialization for subsequent hidden layers\n",
        "        for i in range(2, hiddenlayers + 1):\n",
        "            W[i] = np.random.rand(hiddennodes, hiddennodes) - 0.5\n",
        "            b[i] = np.zeros((hiddennodes, 1))\n",
        "\n",
        "        # Random initialization for the output layer (from last hidden layer to output)\n",
        "        W[hiddenlayers + 1] = np.random.rand(output_nodes, hiddennodes) - 0.5\n",
        "        b[hiddenlayers + 1] = np.zeros((output_nodes, 1))\n",
        "\n",
        "\n",
        "    return W, b\n",
        "\n"
      ],
      "metadata": {
        "id": "ykuw6EsQN1Hi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation"
      ],
      "metadata": {
        "id": "pyTeztydTGPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for applying activation function\n",
        "def g(z, act_func):\n",
        "    if act_func == \"sigmoid\":\n",
        "        return sigmoid(z)\n",
        "    elif act_func == \"tanh\":\n",
        "        return tanh(z)\n",
        "    elif act_func == \"relu\":\n",
        "        return relu(z)\n",
        "\n",
        "# for applying derivative of activation function\n",
        "def g_prime(z, act_func):\n",
        "    if act_func == \"sigmoid\":\n",
        "        return d_sigmoid(z)\n",
        "    elif act_func == \"tanh\":\n",
        "        return d_tanh(z)\n",
        "    elif act_func == \"relu\":\n",
        "        return d_relu(z)\n",
        "\n",
        "# for calculating output\n",
        "def o(z, act_func):\n",
        "    if act_func == \"softmax\":\n",
        "        return softmax(z)\n",
        "\n",
        "# Applying forward propagation for the (L-1) th training example\n",
        "def forward_propagation(x_train, hidden_layers, hidden_nodes, input_nodes, output_nodes, W, b, a, h, L, act_func):\n",
        "    h[0] = x_train[L].reshape(-1, 1)\n",
        "    for layer in range(1, hidden_layers + 1):\n",
        "        a[layer] = np.dot(W[layer], h[layer - 1]) + b[layer]\n",
        "        h[layer] = g(a[layer], act_func)\n",
        "    a[hidden_layers + 1] = np.dot(W[hidden_layers + 1], h[hidden_layers]) + b[hidden_layers + 1]\n",
        "    ycap = o(a[hidden_layers + 1], \"softmax\")\n",
        "    return ycap, a, h\n"
      ],
      "metadata": {
        "id": "VlVPL6vpTJ4j"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}